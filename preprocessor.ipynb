{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer,SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset/trainning-tweets.csv',encoding = \"ISO-8859-1\",names=['sentiment','id','date','Query','user','tweet']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.tweet.values.tolist()\n",
    "y = df.sentiment.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_nolinks = [re.sub(r'http\\S+', '', sentence) for sentence in X]\n",
    "X_nolinks = [re.sub(r'https\\S+', '', sentence) for sentence in X_nolinks]\n",
    "X_nolinks = [re.sub(r'www\\S+', '', sentence) for sentence in X_nolinks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove sitações do twitter como @user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_noRef = [re.sub(r'@\\S+', '', sentence) for sentence in X_nolinks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove caracteres especiais "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = [re.sub(r\"[^a-zA-Z0-9]+\", ' ', sentence.lower()) for sentence in X_noRef]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@heather2711 Good thing I didn't find any then  None of the ones I like come in my size. Stupid big feet \n",
      " good thing i didn t find any then none of the ones i like come in my size stupid big feet \n",
      "DEA's are no fun  \n",
      "dea s are no fun \n",
      "@tommcfly no i haven't  hey but you guys are back in England!!! Welll it was super great last night!! Can't wait for the next gig!\n",
      " no i haven t hey but you guys are back in england welll it was super great last night can t wait for the next gig \n",
      "I googled up remedies for jonzing and reading was one of the suggestions. therefore i'm now reading angels and demons.. it's pretty bunk \n",
      "i googled up remedies for jonzing and reading was one of the suggestions therefore i m now reading angels and demons it s pretty bunk \n",
      "trying to fix my phone         fail\n",
      "trying to fix my phone fail\n",
      "@amazingphoebe mum said you couldn't come over \n",
      " mum said you couldn t come over \n",
      "oh well back to work tomorrow \n",
      "oh well back to work tomorrow \n",
      "Powerpoint on a Sunday morning \n",
      "powerpoint on a sunday morning \n",
      "thinks it's very surreal being back at university and wants to come home already! \n",
      "thinks it s very surreal being back at university and wants to come home already \n",
      "@Fuzzie_74 You're job hunting? \n",
      " you re job hunting \n"
     ]
    }
   ],
   "source": [
    "for i in range(20000,20010):\n",
    "    print(X[i])\n",
    "    #print(X_nolinks[i])\n",
    "    #print(X_noRef[i])\n",
    "    print(X_new[i])\n",
    "    #print(X_new2[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming with PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowball=SnowballStemmer(language=\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def stemSentence(sentence):\n",
    "#     token_words=word_tokenize(sentence)\n",
    "#     token_words\n",
    "#     stem_sentence=[]\n",
    "#     for word in token_words:\n",
    "#         stem_sentence.append(snowball.stem(word))\n",
    "#         stem_sentence.append(\" \")\n",
    "#     return \"\".join(stem_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def lemmSentence(sentence):\n",
    "#     token_words=word_tokenize(sentence)\n",
    "#     token_words\n",
    "#     stem_sentence=[]\n",
    "#     for word in token_words:\n",
    "#         stem_sentence.append(lemmatizer.lemmatize(word))\n",
    "#         stem_sentence.append(\" \")\n",
    "#     return \"\".join(stem_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Próxima linha demora ~10 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_new = []\n",
    "# for sentence in tqdm(X):\n",
    "#     stem_sentence = stemSentence(sentence)\n",
    "#     new_sentence = lemmSentence(sentence)\n",
    "#     X_new.append(new_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caracterização dos textos com TFIDF e remoção de stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB(alpha=0.5)\n",
    "# log_model = LogisticRegression()\n",
    "# stemmer = nltk.stem.RSLPStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[('vect', vect)\n",
    "                       ,('mnb', mnb)\n",
    "                      ])\n",
    "param_grid = { \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "...rue,\n",
       "        vocabulary=None)), ('mnb', MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True))]),\n",
       "       fit_params=None, iid=False, n_jobs=None, param_grid={},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search = GridSearchCV(pipe, param_grid, iid=False, cv=5, return_train_score=False)\n",
    "search.fit(X_noRef, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15.568531</td>\n",
       "      <td>0.916648</td>\n",
       "      <td>3.650883</td>\n",
       "      <td>0.283175</td>\n",
       "      <td>{}</td>\n",
       "      <td>0.752059</td>\n",
       "      <td>0.746534</td>\n",
       "      <td>0.745162</td>\n",
       "      <td>0.755484</td>\n",
       "      <td>0.749356</td>\n",
       "      <td>0.749719</td>\n",
       "      <td>0.003735</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time params  \\\n",
       "0      15.568531      0.916648         3.650883        0.283175     {}   \n",
       "\n",
       "   split0_test_score  split1_test_score  split2_test_score  split3_test_score  \\\n",
       "0           0.752059           0.746534           0.745162           0.755484   \n",
       "\n",
       "   split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "0           0.749356         0.749719        0.003735                1  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = pd.DataFrame(search.cv_results_)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.predict(['Dude, i''m so sad and happy right now!!!'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
